{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31ecde5b-a777-4575-b85a-cdace8c5cdac",
   "metadata": {},
   "source": [
    "# Total Employment Trend in Franklin and Delaware Countires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9474f75-08e1-48e8-acfa-2a06268c5b89",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaabe2c-f4c5-4c5a-bc16-896fa9527eb4",
   "metadata": {},
   "source": [
    "(Blurb Here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63bcaf4-5fc4-473f-b2fb-cb716d7fcacb",
   "metadata": {},
   "source": [
    "### Process Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c87be89-32e4-44e3-af55-c451bee8e02f",
   "metadata": {},
   "source": [
    "(Steps here:)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabf25f4-0682-4eb1-90ad-8fbd5ef70787",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a70168-ed69-48b8-940d-08e0566fd709",
   "metadata": {},
   "source": [
    "### Import required package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cb71c5b7-f061-4772-80ae-03573e131481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import urllib.request\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import json\n",
    "from tableschema import Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05530b6c-fd5c-483c-9a0e-b50051422c1a",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "903e357f-db12-4b5a-af04-c5ef65426180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output directories\n",
    "INPUT_DIR = \"./input_data\"\n",
    "OUTPUT_DIR = \"./output_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e6e88c-5a26-4cb0-9564-d236050ac351",
   "metadata": {},
   "source": [
    "### Define inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388abfe5-6d11-4984-9c66-9ca9da4b2cef",
   "metadata": {},
   "source": [
    "#### (Input title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ee66d1bf-dd1d-4f36-aee7-31cddafd0ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: ./input_data\\combined.csv\n",
      "Schema: ./input_data\\combined_schema.json\n"
     ]
    }
   ],
   "source": [
    "COMBINED_TABLE_FILENAME = \"combined.csv\"\n",
    "COMBINED_TABLE_PATH = os.path.join(INPUT_DIR, COMBINED_TABLE_FILENAME)\n",
    "COMBINED_TABLE_SCHEMA_FILENAME = COMBINED_TABLE_FILENAME.replace(\".csv\",\"_schema.json\")\n",
    "COMBINED_TABLE_SCHEMA_PATH = os.path.join(INPUT_DIR, COMBINED_TABLE_SCHEMA_FILENAME)\n",
    "print(\"Data: {}\".format(COMBINED_TABLE_PATH))\n",
    "print(\"Schema: {}\".format(COMBINED_TABLE_SCHEMA_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cbfba9-4cb4-430b-b78a-9eac4a35754a",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_TABLE_FILENAME = \"compiled.xlsx\"\n",
    "OUTPUT_TABLE_PATH = os.path.join(OUTPUT_DIR, OUTPUT_TABLE_FILENAME)\n",
    "print(\"Output data path: {}\".format(OUTPUT_TABLE_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d399902a-7dfc-418c-b10a-f23d77f71e5b",
   "metadata": {},
   "source": [
    "### Define outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd177fc7-552b-42f0-b5b6-390f953e85c7",
   "metadata": {},
   "source": [
    "#### (Output title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22f07df-c42b-44bb-9d36-3c053e914656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db1345ed-e16a-4069-a5d7-0b9b94931b3b",
   "metadata": {},
   "source": [
    "#### Define inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467fff3c-9945-4abc-8b6e-803ba431d4d2",
   "metadata": {},
   "source": [
    "## Getting input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6dd60c85-405d-45de-a6be-d1436f53debf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for 2014...\n",
      "Fetching data for 2015...\n",
      "Fetching data for 2016...\n",
      "Fetching data for 2017...\n",
      "Fetching data for 2018...\n",
      "Fetching data for 2019...\n",
      "Fetching data for 2020...\n",
      "Fetching data for 2021...\n",
      "Fetching data for 2014...\n",
      "Fetching data for 2015...\n",
      "Fetching data for 2016...\n",
      "Fetching data for 2017...\n",
      "Fetching data for 2018...\n",
      "Fetching data for 2019...\n",
      "Fetching data for 2020...\n",
      "Fetching data for 2021...\n",
      "The CSV files have been combined and saved.\n",
      "Processed data saved to ./input_data\\combined.csv\n",
      "The data has been aggregated by year and saved.\n"
     ]
    }
   ],
   "source": [
    "# *******************************************************************************\n",
    "# qcewCreateDataRows : This function takes a raw csv string and splits it into\n",
    "# a two-dimensional array containing the data and the header row of the csv file\n",
    "# a try/except block is used to handle for both binary and char encoding\n",
    "def qcewCreateDataRows(csv):\n",
    "    dataRows = []\n",
    "    try: dataLines = csv.decode().split('\\r\\n')\n",
    "    except er: dataLines = csv.split('\\r\\n');\n",
    "    for row in dataLines:\n",
    "        dataRows.append(row.split(','))\n",
    "    return dataRows\n",
    "# *******************************************************************************\n",
    "\n",
    "\n",
    "\n",
    "# *******************************************************************************\n",
    "# qcewGetAreaData : This function takes a year, quarter, and area argument and\n",
    "# returns an array containing the associated area data. Use 'a' for annual\n",
    "# averages. \n",
    "# For all area codes and titles see:\n",
    "# http://www.bls.gov/cew/doc/titles/area/area_titles.htm\n",
    "#\n",
    "def qcewGetAreaData(year,qtr,area):\n",
    "    urlPath = \"http://data.bls.gov/cew/data/api/[YEAR]/[QTR]/area/[AREA].csv\"\n",
    "    urlPath = urlPath.replace(\"[YEAR]\",year)\n",
    "    urlPath = urlPath.replace(\"[QTR]\",qtr.lower())\n",
    "    urlPath = urlPath.replace(\"[AREA]\",area.upper())\n",
    "    httpStream = urllib.request.urlopen(urlPath)\n",
    "    csv = httpStream.read()\n",
    "    httpStream.close()\n",
    "    return qcewCreateDataRows(csv)\n",
    "\n",
    "\n",
    "def fetch_and_combine_qcew_data(start_year, end_year, qtr, area):\n",
    "    all_years_data = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        print(f\"Fetching data for {year}...\")\n",
    "        year_data = qcewGetAreaData(str(year), qtr, area)\n",
    "        columns_no_quotes = [col.replace('\"', '') for col in year_data[0]]\n",
    "        year_data_df = pd.DataFrame(year_data[1:], columns=columns_no_quotes)\n",
    "        # Add a column to distinguish data by year\n",
    "        year_data_df['year'] = year\n",
    "        # Remove every instance of \" in the data\n",
    "        year_data_df = year_data_df.replace('\"', '', regex=True)\n",
    "        all_years_data.append(year_data_df)\n",
    "        \n",
    "    # Combine all DataFrame objects into a single DataFrame\n",
    "    combined_df = pd.concat(all_years_data, ignore_index=True)\n",
    "    \n",
    "    # Apply filtering: retain rows where industry_code == \"10\" and own_code == \"0\"\n",
    "    filtered_df = combined_df.loc[(combined_df['industry_code'] == \"10\") & (combined_df['own_code'] == \"0\")]\n",
    "\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "# Fetch and combine the data\n",
    "combined_franklin_data = fetch_and_combine_qcew_data(2014, 2021, \"a\", \"39049\")\n",
    "\n",
    "# Fetch and combine the data\n",
    "combined_delaware_data = fetch_and_combine_qcew_data(2014, 2021, \"a\", \"39041\")\n",
    "\n",
    "# Combine the DataFrames\n",
    "combined_df = pd.concat([combined_franklin_data, combined_delaware_data], ignore_index=True)\n",
    "\n",
    "combined_df.to_csv(COMBINED_TABLE_PATH, index=False)\n",
    "\n",
    "print(\"The CSV files have been combined and saved.\")\n",
    "print(f\"Processed data saved to {COMBINED_TABLE_PATH}\")\n",
    "\n",
    "\n",
    "# Create table\n",
    "table = Table(COMBINED_TABLE_PATH)\n",
    "\n",
    "# Infer table variable types\n",
    "table.infer()\n",
    "\n",
    "# Convert missing values to 'N/A'\n",
    "table.schema.descriptor['missingValues'] = ['N/A', '']\n",
    "table.schema.commit()\n",
    "\n",
    "# Save schema\n",
    "table.schema.save(COMBINED_TABLE_SCHEMA_PATH)\n",
    "\n",
    "\n",
    "\n",
    "# Load the schema from the JSON file\n",
    "with open(COMBINED_TABLE_SCHEMA_PATH, 'r') as file:\n",
    "    schema = json.load(file)\n",
    "\n",
    "# Define a function to convert data types based on the schema\n",
    "def convert_data_types(row, schema):\n",
    "    converted_row = {}\n",
    "    for field in schema['fields']:\n",
    "        field_name = field['name']\n",
    "        field_type = field['type']\n",
    "        if field_name in row:\n",
    "            if field_type == 'int':\n",
    "                converted_row[field_name] = int(row[field_name])\n",
    "            elif field_type == 'string':\n",
    "                converted_row[field_name] = str(row[field_name])\n",
    "            # Add more type conversions as needed\n",
    "            else:\n",
    "                converted_row[field_name] = row[field_name]  # Keep as is if type not recognized\n",
    "    return converted_row\n",
    "\n",
    "# Read the input data, apply conversion, and process it\n",
    "converted_data = []\n",
    "\n",
    "with open(COMBINED_TABLE_PATH, 'r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        converted_row = convert_data_types(row, schema)\n",
    "        converted_data.append(converted_row)\n",
    "\n",
    "# Preview migration change type table\n",
    "df = pd.read_csv(COMBINED_TABLE_PATH)\n",
    "df.head()\n",
    "\n",
    "# Aggregate rows by \"year\" and sum up all other numeric columns\n",
    "aggregated_df = df.groupby('year').sum().reset_index()\n",
    "\n",
    "aggregated_df.to_csv(COMBINED_TABLE_PATH, index=False)\n",
    "\n",
    "print(\"The data has been aggregated by year and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6fd420-5ba5-4850-8fd5-d26686b83a95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e887833f-91f5-4e9b-b3f5-fb959faabdf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
